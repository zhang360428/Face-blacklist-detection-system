import os
import json
import time
import logging
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from sklearn.metrics import (
    confusion_matrix, roc_curve, precision_recall_curve, auc,
    accuracy_score, precision_score, recall_score, f1_score
)

from face_model import FaceRecognitionModel
from milvus_client import MilvusClient
from config import THRESHOLD, TEST_RESULTS_PATH

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class EvaluationVisualizer:
    """è¯„ä¼°ç»“æœå¯è§†åŒ–å·¥å…·ç±»"""
    
    def __init__(self, save_dir: str = "evaluation_results"):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
        # è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        plt.style.use('seaborn-v0_8-darkgrid')
    
    def plot_confusion_matrix(self, y_true: List[int], y_pred: List[int], 
                            title: str = "matrix", save_path: Optional[str] = None):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾"""
        cm = confusion_matrix(y_true, y_pred)
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        fig, ax = plt.subplots(figsize=(10, 8))
        
        # ä½¿ç”¨ seaborn ç»˜åˆ¶çƒ­åŠ›å›¾
        sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', 
                    xticklabels=['negtive', 'postive'], yticklabels=['negtive', 'postive'],
                    ax=ax, cbar_kws={'label': 'ratio'})
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                count = cm[i, j]
                ax.text(j + 0.5, i + 0.7, f'\n({count})', 
                       ha='center', va='center', fontsize=10)
        
        ax.set_xlabel('predict', fontsize=12)
        ax.set_ylabel('truth', fontsize=12)
        ax.set_title(f'{title}\n(threshold={THRESHOLD})', fontsize=14, fontweight='bold')
        
        # æ·»åŠ å›¾ä¾‹è¯´æ˜
        plt.figtext(0.5, -0.05, 
                   "Positive samples: should be in the blacklist\nNegative samples: should not be in the blacklist", 
                   ha='center', fontsize=10, 
                   bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³: {save_path}")
        
        plt.show()
        plt.close()
    
    def plot_roc_curve(self, y_true: List[int], y_scores: List[float], 
                      title: str = "ROCæ›²çº¿", save_path: Optional[str] = None):
        """ç»˜åˆ¶ROCæ›²çº¿"""
        fpr, tpr, thresholds = roc_curve(y_true, y_scores)
        roc_auc = auc(fpr, tpr)
        
        fig, ax = plt.subplots(figsize=(10, 8))
        
        ax.plot(fpr, tpr, color='darkorange', lw=2, 
                label=f'ROCæ›²çº¿ (AUC = {roc_auc:.3f})')
        ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', 
                label='éšæœºçŒœæµ‹')
        
        # æ ‡è®°å½“å‰é˜ˆå€¼ç‚¹
        current_threshold_idx = np.argmin(np.abs(thresholds - THRESHOLD))
        ax.plot(fpr[current_threshold_idx], tpr[current_threshold_idx], 
                'ro', markersize=10, label=f'å½“å‰é˜ˆå€¼ ({THRESHOLD})')
        
        ax.set_xlabel('å‡é˜³æ€§ç‡ (FPR)', fontsize=12)
        ax.set_ylabel('çœŸé˜³æ€§ç‡ (TPR)', fontsize=12)
        ax.set_title(title, fontsize=14, fontweight='bold')
        ax.legend(loc="lower right", fontsize=11)
        ax.grid(alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"ROCæ›²çº¿å·²ä¿å­˜è‡³: {save_path}")
        
        plt.show()
        plt.close()
        
        return roc_auc
    
    def plot_pr_curve(self, y_true: List[int], y_scores: List[float], 
                     title: str = "ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿", save_path: Optional[str] = None):
        """ç»˜åˆ¶PRæ›²çº¿"""
        precision, recall, thresholds = precision_recall_curve(y_true, y_scores)
        pr_auc = auc(recall, precision)
        
        # è®¡ç®—åŸºå‡†çº¿ï¼ˆæ­£æ ·æœ¬æ¯”ä¾‹ï¼‰
        baseline = np.mean(y_true)
        
        fig, ax = plt.subplots(figsize=(10, 8))
        
        ax.plot(recall, precision, color='darkorange', lw=2, 
                label=f'PRæ›²çº¿ (AUC = {pr_auc:.3f})')
        ax.axhline(y=baseline, color='navy', lw=2, linestyle='--', 
                  label=f'åŸºå‡†çº¿ (æ­£æ ·æœ¬æ¯”ä¾‹ = {baseline:.3f})')
        
        # æ ‡è®°å½“å‰é˜ˆå€¼ç‚¹
        current_threshold_idx = np.argmin(np.abs(thresholds - THRESHOLD))
        if current_threshold_idx < len(recall):
            ax.plot(recall[current_threshold_idx], precision[current_threshold_idx], 
                    'ro', markersize=10, label=f'å½“å‰é˜ˆå€¼ ({THRESHOLD})')
        
        ax.set_xlabel('å¬å›ç‡ (Recall)', fontsize=12)
        ax.set_ylabel('ç²¾ç¡®ç‡ (Precision)', fontsize=12)
        ax.set_title(title, fontsize=14, fontweight='bold')
        ax.legend(loc="lower left", fontsize=11)
        ax.grid(alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"PRæ›²çº¿å·²ä¿å­˜è‡³: {save_path}")
        
        plt.show()
        plt.close()
        
        return pr_auc
    
    def plot_similarity_distribution(self, positive_scores: List[float], 
                                   negative_scores: List[float],
                                   threshold: float = THRESHOLD,
                                   save_path: Optional[str] = None):
        """ç»˜åˆ¶ç›¸ä¼¼åº¦åˆ†å¸ƒç›´æ–¹å›¾"""
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # ç»˜åˆ¶åˆ†å¸ƒå›¾
        ax.hist(positive_scores, bins=50, alpha=0.6, label='æ­£æ ·æœ¬', 
                color='green', density=True)
        ax.hist(negative_scores, bins=50, alpha=0.6, label='è´Ÿæ ·æœ¬', 
                color='red', density=True)
        
        # æ·»åŠ é˜ˆå€¼çº¿
        ax.axvline(x=threshold, color='blue', linestyle='--', 
                  linewidth=2, label=f'å†³ç­–é˜ˆå€¼ ({threshold})')
        
        ax.set_xlabel('ç›¸ä¼¼åº¦å¾—åˆ†', fontsize=12)
        ax.set_ylabel('å¯†åº¦', fontsize=12)
        ax.set_title('ç›¸ä¼¼åº¦åˆ†å¸ƒå›¾', fontsize=14, fontweight='bold')
        ax.legend(fontsize=11)
        ax.grid(alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        stats_text = f"æ­£æ ·æœ¬: Î¼={np.mean(positive_scores):.3f}, Ïƒ={np.std(positive_scores):.3f}\n"
        stats_text += f"è´Ÿæ ·æœ¬: Î¼={np.mean(negative_scores):.3f}, Ïƒ={np.std(negative_scores):.3f}"
        ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, 
                fontsize=10, verticalalignment='top',
                bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"ç›¸ä¼¼åº¦åˆ†å¸ƒå›¾å·²ä¿å­˜è‡³: {save_path}")
        
        plt.show()
        plt.close()
    
    def plot_threshold_analysis(self, thresholds: List[float], metrics: Dict[str, List[float]],
                              save_path: Optional[str] = None):
        """ç»˜åˆ¶ä¸åŒé˜ˆå€¼ä¸‹çš„æŒ‡æ ‡å˜åŒ–å›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        axes = axes.flatten()
        
        # ç»˜åˆ¶å‡†ç¡®ç‡
        if 'accuracy' in metrics:
            axes[0].plot(thresholds, metrics['accuracy'], 'b-', linewidth=2)
            axes[0].axvline(x=THRESHOLD, color='r', linestyle='--', label='å½“å‰é˜ˆå€¼')
            axes[0].set_xlabel('é˜ˆå€¼', fontsize=11)
            axes[0].set_ylabel('å‡†ç¡®ç‡', fontsize=11)
            axes[0].set_title('å‡†ç¡®ç‡ vs é˜ˆå€¼', fontsize=12, fontweight='bold')
            axes[0].legend()
            axes[0].grid(alpha=0.3)
        
        # ç»˜åˆ¶ç²¾ç¡®ç‡å’Œå¬å›ç‡
        if 'precision' in metrics and 'recall' in metrics:
            axes[1].plot(thresholds, metrics['precision'], 'b-', linewidth=2, label='ç²¾ç¡®ç‡')
            axes[1].plot(thresholds, metrics['recall'], 'g-', linewidth=2, label='å¬å›ç‡')
            axes[1].axvline(x=THRESHOLD, color='r', linestyle='--', label='å½“å‰é˜ˆå€¼')
            axes[1].set_xlabel('é˜ˆå€¼', fontsize=11)
            axes[1].set_ylabel('æŒ‡æ ‡å€¼', fontsize=11)
            axes[1].set_title('ç²¾ç¡®ç‡ & å¬å›ç‡ vs é˜ˆå€¼', fontsize=12, fontweight='bold')
            axes[1].legend()
            axes[1].grid(alpha=0.3)
        
        # ç»˜åˆ¶FPRå’ŒFNR
        if 'fpr' in metrics and 'fnr' in metrics:
            axes[2].plot(thresholds, metrics['fpr'], 'r-', linewidth=2, label='FPR')
            axes[2].plot(thresholds, metrics['fnr'], 'm-', linewidth=2, label='FNR')
            axes[2].axvline(x=THRESHOLD, color='r', linestyle='--', label='å½“å‰é˜ˆå€¼')
            axes[2].set_xlabel('é˜ˆå€¼', fontsize=11)
            axes[2].set_ylabel('é”™è¯¯ç‡', fontsize=11)
            axes[2].set_title('è¯¯æŠ¥ç‡ & æ¼æŠ¥ç‡ vs é˜ˆå€¼', fontsize=12, fontweight='bold')
            axes[2].legend()
            axes[2].grid(alpha=0.3)
        
        # ç»˜åˆ¶F1åˆ†æ•°
        if 'f1_score' in metrics:
            axes[3].plot(thresholds, metrics['f1_score'], 'y-', linewidth=2, label='F1åˆ†æ•°')
            axes[3].axvline(x=THRESHOLD, color='r', linestyle='--', label='å½“å‰é˜ˆå€¼')
            axes[3].set_xlabel('é˜ˆå€¼', fontsize=11)
            axes[3].set_ylabel('F1åˆ†æ•°', fontsize=11)
            axes[3].set_title('F1åˆ†æ•° vs é˜ˆå€¼', fontsize=12, fontweight='bold')
            axes[3].legend()
            axes[3].grid(alpha=0.3)
        
        plt.suptitle('é˜ˆå€¼æ•æ„Ÿæ€§åˆ†æ', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"é˜ˆå€¼åˆ†æå›¾å·²ä¿å­˜è‡³: {save_path}")
        
        plt.show()
        plt.close()


class BlacklistEvaluator:
    """é»‘åå•ç³»ç»Ÿè¯„ä¼°å™¨"""
    
    def __init__(self, model: Optional[FaceRecognitionModel] = None, 
                 milvus: Optional[MilvusClient] = None,
                 save_dir: str = "evaluation_results"):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            model: äººè„¸è¯†åˆ«æ¨¡å‹å®ä¾‹
            milvus: Milvuså®¢æˆ·ç«¯å®ä¾‹
            save_dir: ç»“æœä¿å­˜ç›®å½•
        """
        self.model = model or FaceRecognitionModel()
        self.milvus = milvus or MilvusClient()
        self.visualizer = EvaluationVisualizer(save_dir)
        self.save_dir = save_dir
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        os.makedirs(os.path.join(save_dir, "plots"), exist_ok=True)
        os.makedirs(os.path.join(save_dir, "reports"), exist_ok=True)
        
        logger.info(f"è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆï¼Œç»“æœå°†ä¿å­˜è‡³: {save_dir}")
    
    def evaluate_test_set(self, test_set_path: str = "test_set_info.json",
                         generate_plots: bool = True) -> Dict[str, Any]:
        """
        è¯„ä¼°æµ‹è¯•é›†æ€§èƒ½
        
        Args:
            test_set_path: æµ‹è¯•é›†æ–‡ä»¶è·¯å¾„
            generate_plots: æ˜¯å¦ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        if not os.path.exists(test_set_path):
            raise FileNotFoundError(f"æµ‹è¯•é›†æ–‡ä»¶ {test_set_path} ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ build_blacklist.py")
        
        # åŠ è½½æµ‹è¯•é›†
        logger.info("åŠ è½½æµ‹è¯•é›†æ•°æ®...")
        with open(test_set_path, "r", encoding='utf-8') as f:
            test_set = json.load(f)
        
        logger.info(f"æµ‹è¯•é›†åŠ è½½å®Œæˆ: {len(test_set.get('positive_samples', []))} æ­£æ ·æœ¬, "
                   f"{len(test_set.get('negative_samples', []))} è´Ÿæ ·æœ¬")
        
        start_time = time.time()
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "threshold": THRESHOLD,
            "positive_samples": [],
            "negative_samples": [],
            "metrics": {},
            "summary": {}
        }
        
        # è¯„ä¼°æ ·æœ¬
        logger.info("å¼€å§‹è¯„ä¼°æ­£æ ·æœ¬...")
        for sample in tqdm(test_set["positive_samples"], desc="æ­£æ ·æœ¬è¯„ä¼°"):
            self._evaluate_sample(sample, results["positive_samples"])
        
        logger.info("å¼€å§‹è¯„ä¼°è´Ÿæ ·æœ¬...")
        for sample in tqdm(test_set["negative_samples"], desc="è´Ÿæ ·æœ¬è¯„ä¼°"):
            self._evaluate_sample(sample, results["negative_samples"])
        
        # è®¡ç®—æŒ‡æ ‡
        logger.info("è®¡ç®—æ€§èƒ½æŒ‡æ ‡...")
        metrics = self._calculate_detailed_metrics(results)
        results["metrics"] = metrics
        
        # ä¿å­˜ç»“æœ
        self._save_results(results)
        
        # ç”Ÿæˆå¯è§†åŒ–
        if generate_plots:
            logger.info("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
            self._generate_visualizations(results)
        
        total_time = time.time() - start_time
        results["total_time"] = total_time
        
        logger.info(f"è¯„ä¼°å®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f}ç§’")
        self._print_summary(metrics)
        
        return results
    
    def _evaluate_sample(self, sample: Dict[str, Any], results_list: List[Dict[str, Any]]):
        """è¯„ä¼°å•ä¸ªæ ·æœ¬"""
        image_path = sample["image_path"]
        expected_in_blacklist = sample["expected_in_blacklist"]
        person_name = sample.get("person_name", "Unknown")
        
        # æå–ç‰¹å¾
        success, feature, _ = self.model.extract_feature(image_path)
        
        if not success:
            results_list.append({
                "person_name": person_name,
                "image_path": image_path,
                "expected_in_blacklist": expected_in_blacklist,
                "detected": False,
                "predicted_in_blacklist": False,
                "similarity": 0.0,
                "correct": False,
                "error": "æœªæ£€æµ‹åˆ°äººè„¸"
            })
            return
        
        # æœç´¢é»‘åå•
        is_match, matched_person, similarity, face_id = self.milvus.search_face(feature)
        
        # åˆ¤æ–­æ˜¯å¦å‘½ä¸­é»‘åå•
        predicted_in_blacklist = is_match and similarity >= THRESHOLD
        
        # åˆ¤æ–­æ˜¯å¦æ­£ç¡®
        correct = (predicted_in_blacklist == expected_in_blacklist)
        
        results_list.append({
            "person_name": person_name,
            "image_path": image_path,
            "expected_in_blacklist": expected_in_blacklist,
            "detected": True,
            "predicted_in_blacklist": predicted_in_blacklist,
            "matched_person": matched_person if is_match else None,
            "matched_face_id": face_id if is_match else None,
            "similarity": similarity,
            "correct": correct
        })
    
    def _calculate_detailed_metrics(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡"""
        # æå–æ•°æ®
        positive_samples = results["positive_samples"]
        negative_samples = results["negative_samples"]
        
        # çœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾
        y_true = ([1] * len(positive_samples) + 
                 [0] * len(negative_samples))
        y_pred = ([1 if s["predicted_in_blacklist"] else 0 
                  for s in positive_samples] +
                 [1 if s["predicted_in_blacklist"] else 0 
                  for s in negative_samples])
        y_scores = ([s["similarity"] for s in positive_samples] +
                   [s["similarity"] for s in negative_samples])
        
        # ç»Ÿè®¡åŸºç¡€æ•°æ®
        TP = sum(1 for s in positive_samples if s["predicted_in_blacklist"])
        FN = len(positive_samples) - TP
        FP = sum(1 for s in negative_samples if s["predicted_in_blacklist"])
        TN = len(negative_samples) - FP
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = (TP + TN) / (TP + FP + TN + FN) if (TP + FP + TN + FN) > 0 else 0
        precision = TP / (TP + FP) if (TP + FP) > 0 else 0
        recall = TP / (TP + FN) if (TP + FN) > 0 else 0
        f1_score_val = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        fpr = FP / (FP + TN) if (FP + TN) > 0 else 0
        fnr = FN / (TP + FN) if (TP + FN) > 0 else 0
        
        # è®¡ç®—æ¯å¼ å›¾ç‰‡å¹³å‡è€—æ—¶
        total_images = len(positive_samples) + len(negative_samples)
        avg_time_per_image = results.get("total_time", 0) / total_images if total_images > 0 else 0
        
        # è®¡ç®—æ ·æœ¬ç»Ÿè®¡ä¿¡æ¯
        positive_similarities = [s["similarity"] for s in positive_samples if s["detected"]]
        negative_similarities = [s["similarity"] for s in negative_samples if s["detected"]]
        
        metrics = {
            # åŸºæœ¬ç»Ÿè®¡
            "total_samples": total_images,
            "positive_samples": len(positive_samples),
            "negative_samples": len(negative_samples),
            "detected_faces": len(positive_samples) + len(negative_samples) - 
                            sum(1 for s in positive_samples + negative_samples if not s["detected"]),
            
            # æ··æ·†çŸ©é˜µ
            "TP": TP,
            "TN": TN,
            "FP": FP,
            "FN": FN,
            
            # æ ¸å¿ƒæŒ‡æ ‡
            "accuracy": accuracy * 100,
            "precision": precision * 100,
            "recall": recall * 100,
            "f1_score": f1_score_val * 100,
            "false_positive_rate": fpr * 100,
            "false_negative_rate": fnr * 100,
            
            # æ—¶é—´æ€§èƒ½
            "avg_time_per_image_ms": avg_time_per_image * 1000,
            
            # ç›¸ä¼¼åº¦ç»Ÿè®¡
            "positive_similarity_mean": np.mean(positive_similarities) if positive_similarities else 0,
            "positive_similarity_std": np.std(positive_similarities) if positive_similarities else 0,
            "negative_similarity_mean": np.mean(negative_similarities) if negative_similarities else 0,
            "negative_similarity_std": np.std(negative_similarities) if negative_similarities else 0,
        }
        
        # å­˜å‚¨ç”¨äºå¯è§†åŒ–çš„æ•°æ®
        results["y_true"] = y_true
        results["y_pred"] = y_pred
        results["y_scores"] = y_scores
        
        return metrics
    
    def _save_results(self, results: Dict[str, Any]):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        # ä¿å­˜å®Œæ•´ç»“æœ
        results_path = os.path.join(self.save_dir, "reports", "evaluation_results.json")
        with open(results_path, "w", encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜ç®€æ´æŠ¥å‘Š
        summary_path = os.path.join(self.save_dir, "reports", "summary.json")
        summary = {
            "timestamp": results["timestamp"],
            "threshold": results["threshold"],
            "metrics": results["metrics"]
        }
        with open(summary_path, "w", encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {self.save_dir}")
    
    def _generate_visualizations(self, results: Dict[str, Any]):
        """ç”Ÿæˆæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨"""
        plots_dir = os.path.join(self.save_dir, "plots")
        
        # å‡†å¤‡æ•°æ®
        y_true = results["y_true"]
        y_pred = results["y_pred"]
        y_scores = results["y_scores"]
        
        positive_scores = [s["similarity"] for s in results["positive_samples"] if s["detected"]]
        negative_scores = [s["similarity"] for s in results["negative_samples"] if s["detected"]]
        
        # 1. æ··æ·†çŸ©é˜µ
        self.visualizer.plot_confusion_matrix(
            y_true, y_pred,
            title="é»‘åå•ç³»ç»Ÿæ··æ·†çŸ©é˜µ",
            save_path=os.path.join(plots_dir, "confusion_matrix.png")
        )
        
        # 2. ROCæ›²çº¿
        roc_auc = self.visualizer.plot_roc_curve(
            y_true, y_scores,
            title="é»‘åå•ç³»ç»ŸROCæ›²çº¿",
            save_path=os.path.join(plots_dir, "roc_curve.png")
        )
        results["metrics"]["roc_auc"] = roc_auc
        
        # 3. PRæ›²çº¿
        pr_auc = self.visualizer.plot_pr_curve(
            y_true, y_scores,
            title="é»‘åå•ç³»ç»ŸPRæ›²çº¿",
            save_path=os.path.join(plots_dir, "pr_curve.png")
        )
        results["metrics"]["pr_auc"] = pr_auc
        
        # 4. ç›¸ä¼¼åº¦åˆ†å¸ƒ
        self.visualizer.plot_similarity_distribution(
            positive_scores, negative_scores,
            threshold=THRESHOLD,
            save_path=os.path.join(plots_dir, "similarity_distribution.png")
        )
        
        logger.info("æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ")
    
    def search_optimal_threshold(self, test_set_path: str = "test_set_info.json",
                                threshold_range: Tuple[float, float] = (0.3, 0.9),
                                step: float = 0.01) -> Dict[float, Dict[str, float]]:
        """
        æœç´¢æœ€ä¼˜é˜ˆå€¼
        
        Args:
            test_set_path: æµ‹è¯•é›†è·¯å¾„
            threshold_range: é˜ˆå€¼æœç´¢èŒƒå›´
            step: é˜ˆå€¼æ­¥é•¿
        
        Returns:
            ä¸åŒé˜ˆå€¼ä¸‹çš„æ€§èƒ½æŒ‡æ ‡
        """
        logger.info(f"å¼€å§‹æœç´¢æœ€ä¼˜é˜ˆå€¼ï¼ŒèŒƒå›´: {threshold_range}, æ­¥é•¿: {step}")
        
        # åŠ è½½æµ‹è¯•é›†
        with open(test_set_path, "r", encoding='utf-8') as f:
            test_set = json.load(f)
        
        # æå‰æå–æ‰€æœ‰ç‰¹å¾
        all_samples = []
        all_features = []
        all_labels = []
        
        logger.info("é¢„æå–æ‰€æœ‰æ ·æœ¬ç‰¹å¾...")
        for sample in test_set["positive_samples"] + test_set["negative_samples"]:
            image_path = sample["image_path"]
            expected = sample["expected_in_blacklist"]
            
            success, feature, _ = self.model.extract_feature(image_path)
            if success:
                all_samples.append(sample)
                all_features.append(feature)
                all_labels.append(1 if expected else 0)
        
        # æœç´¢é˜ˆå€¼
        thresholds = np.arange(threshold_range[0], threshold_range[1] + step, step)
        results = {}
        
        for threshold in tqdm(thresholds, desc="é˜ˆå€¼æœç´¢"):
            y_pred = []
            
            for feature, label in zip(all_features, all_labels):
                is_match, _, similarity, _ = self.milvus.search_face(feature)
                predicted = is_match and similarity >= threshold
                y_pred.append(1 if predicted else 0)
            
            # è®¡ç®—æŒ‡æ ‡
            TP = sum(1 for p, t in zip(y_pred, all_labels) if p == 1 and t == 1)
            FP = sum(1 for p, t in zip(y_pred, all_labels) if p == 1 and t == 0)
            TN = sum(1 for p, t in zip(y_pred, all_labels) if p == 0 and t == 0)
            FN = sum(1 for p, t in zip(y_pred, all_labels) if p == 0 and t == 1)
            
            accuracy = (TP + TN) / (TP + FP + TN + FN) if (TP + FP + TN + FN) > 0 else 0
            precision = TP / (TP + FP) if (TP + FP) > 0 else 0
            recall = TP / (TP + FN) if (TP + FN) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            fpr = FP / (FP + TN) if (FP + TN) > 0 else 0
            fnr = FN / (TP + FN) if (TP + FN) > 0 else 0
            
            results[threshold] = {
                "accuracy": accuracy * 100,
                "precision": precision * 100,
                "recall": recall * 100,
                "f1_score": f1 * 100,
                "fpr": fpr * 100,
                "fnr": fnr * 100
            }
        
        # æ‰¾åˆ°æœ€ä½³F1åˆ†æ•°å¯¹åº”çš„é˜ˆå€¼
        best_threshold = max(results.keys(), key=lambda k: results[k]["f1_score"])
        best_metrics = results[best_threshold]
        
        logger.info(f"æœ€ä¼˜é˜ˆå€¼: {best_threshold:.3f} (F1={best_metrics['f1_score']:.2f}%)")
        
        # å¯è§†åŒ–é˜ˆå€¼æœç´¢
        self._plot_threshold_search_results(results, plots_dir=os.path.join(self.save_dir, "plots"))
        
        return results, best_threshold, best_metrics
    
    def _plot_threshold_search_results(self, threshold_results: Dict[float, Dict[str, float]], 
                                     plots_dir: str):
        """ç»˜åˆ¶é˜ˆå€¼æœç´¢ç»“æœå›¾"""
        thresholds = sorted(threshold_results.keys())
        metrics_data = {metric: [threshold_results[t][metric] for t in thresholds] 
                       for metric in ['accuracy', 'precision', 'recall', 'f1_score', 'fpr', 'fnr']}
        
        plot_path = os.path.join(plots_dir, "threshold_analysis.png")
        self.visualizer.plot_threshold_analysis(thresholds, metrics_data, save_path=plot_path)
    
    def _print_summary(self, metrics: Dict[str, Any]):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        print("\n" + "=" * 70)
        print("é»‘åå•ç³»ç»Ÿæ€§èƒ½è¯„ä¼°æŠ¥å‘Š")
        print("=" * 70)
        print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"å†³ç­–é˜ˆå€¼: {metrics.get('threshold', THRESHOLD)}")
        print("-" * 70)
        print(f"æµ‹è¯•æ ·æœ¬æ€»æ•°: {metrics['total_samples']:,}")
        print(f"æ­£æ ·æœ¬æ•° (åº”åœ¨é»‘åå•): {metrics['positive_samples']:,}")
        print(f"è´Ÿæ ·æœ¬æ•° (ä¸åº”åœ¨é»‘åå•): {metrics['negative_samples']:,}")
        print(f"æˆåŠŸæ£€æµ‹äººè„¸: {metrics['detected_faces']:,}")
        print("-" * 70)
        print(f"å‡†ç¡®ç‡ (Accuracy): {metrics['accuracy']:.2f}%")
        print(f"ç²¾ç¡®ç‡ (Precision): {metrics['precision']:.2f}%")
        print(f"å¬å›ç‡ (Recall): {metrics['recall']:.2f}%")
        print(f"F1åˆ†æ•°: {metrics['f1_score']:.2f}%")
        print("-" * 70)
        print(f"è¯¯æŠ¥ç‡ (FPR): {metrics['false_positive_rate']:.2f}%")
        print(f"æ¼æŠ¥ç‡ (FNR): {metrics['false_negative_rate']:.2f}%")
        if 'roc_auc' in metrics:
            print(f"ROC-AUC: {metrics['roc_auc']:.3f}")
        if 'pr_auc' in metrics:
            print(f"PR-AUC: {metrics['pr_auc']:.3f}")
        print("-" * 70)
        print(f"å¹³å‡è¯†åˆ«æ—¶é—´: {metrics['avg_time_per_image_ms']:.2f}ms/å¼ ")
        print("=" * 70)
        print("\nå…³é”®æŒ‡æ ‡è¯´æ˜:")
        print("- è¯¯æŠ¥ç‡ (FPR): ä¸åœ¨é»‘åå•çš„äººè¢«é”™è¯¯è¯†åˆ«ä¸ºåœ¨é»‘åå•çš„æ¯”ä¾‹")
        print("- æ¼æŠ¥ç‡ (FNR): åœ¨é»‘åå•çš„äººæœªè¢«è¯†åˆ«çš„æ¯”ä¾‹")
        print("- å¬å›ç‡: åœ¨é»‘åå•çš„äººè¢«æ­£ç¡®è¯†åˆ«çš„æ¯”ä¾‹")
        print("- ç²¾ç¡®ç‡: è¢«è¯†åˆ«ä¸ºé»‘åå•çš„äººä¸­çœŸæ­£åœ¨é»‘åå•çš„æ¯”ä¾‹")
        print("=" * 70)
    
    def analyze_errors(self, results_path: Optional[str] = None):
        """
        åˆ†æé”™è¯¯æ¡ˆä¾‹
        
        Args:
            results_path: ç»“æœæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æœ€æ–°çš„ç»“æœ
        """
        if results_path is None:
            results_path = os.path.join(self.save_dir, "reports", "evaluation_results.json")
        
        if not os.path.exists(results_path):
            logger.error("ç»“æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•åˆ†æé”™è¯¯")
            return
        
        with open(results_path, "r", encoding='utf-8') as f:
            results = json.load(f)
        
        print("\n" + "=" * 70)
        print("é”™è¯¯æ¡ˆä¾‹åˆ†æ")
        print("=" * 70)
        
        # å‡é˜³æ€§ï¼ˆè¯¯æŠ¥ï¼‰
        false_positives = [s for s in results["negative_samples"] if not s["correct"]]
        print(f"\nå‡é˜³æ€§æ¡ˆä¾‹ (è¯¯æŠ¥): {len(false_positives)} ä¸ª")
        print("-" * 70)
        for i, fp in enumerate(false_positives[:10]):  # æ˜¾ç¤ºå‰10ä¸ª
            print(f"{i + 1:2d}. {fp['person_name']}")
            print(f"    å›¾ç‰‡: {fp['image_path']}")
            print(f"    åŒ¹é…åˆ°: {fp.get('matched_person', 'Unknown')}")
            print(f"    ç›¸ä¼¼åº¦: {fp['similarity']:.4f}")
        
        # å‡é˜´æ€§ï¼ˆæ¼æŠ¥ï¼‰
        false_negatives = [s for s in results["positive_samples"] if not s["correct"]]
        print(f"\nå‡é˜´æ€§æ¡ˆä¾‹ (æ¼æŠ¥): {len(false_negatives)} ä¸ª")
        print("-" * 70)
        for i, fn in enumerate(false_negatives[:10]):  # æ˜¾ç¤ºå‰10ä¸ª
            print(f"{i + 1:2d}. {fn['person_name']}")
            print(f"    å›¾ç‰‡: {fn['image_path']}")
            print(f"    åŒ¹é…åˆ°: {fn.get('matched_person', 'None')}")
            print(f"    ç›¸ä¼¼åº¦: {fn['similarity']:.4f}")
        
        # æœªæ£€æµ‹åˆ°äººè„¸çš„æ¡ˆä¾‹
        no_face_samples = [s for s in results["positive_samples"] + results["negative_samples"]
                          if not s.get("detected", True)]
        print(f"\næœªæ£€æµ‹åˆ°äººè„¸: {len(no_face_samples)} å¼ ")
        if no_face_samples:
            print("-" * 70)
            for i, sample in enumerate(no_face_samples[:5]):
                print(f"{i + 1}. {sample['person_name']}: {sample['image_path']}")
        
        # ç»Ÿè®¡é”™è¯¯åŸå› 
        print("\n" + "=" * 70)
        print("é”™è¯¯ç»Ÿè®¡æ‘˜è¦")
        print("=" * 70)
        print(f"æ€»é”™è¯¯æ•°: {len(false_positives) + len(false_negatives)}")
        print(f"è¯¯æŠ¥ç‡: {len(false_positives) / len(results['negative_samples']) * 100:.2f}%")
        print(f"æ¼æŠ¥ç‡: {len(false_negatives) / len(results['positive_samples']) * 100:.2f}%")
        print(f"æ£€æµ‹å¤±è´¥ç‡: {len(no_face_samples) / (len(results['positive_samples']) + len(results['negative_samples'])) * 100:.2f}%")
        print("=" * 70)
        
        return false_positives, false_negatives, no_face_samples
    
    def generate_html_report(self, results_path: Optional[str] = None, 
                           output_path: Optional[str] = None):
        """
        ç”ŸæˆHTMLæ ¼å¼æŠ¥å‘Š
        
        Args:
            results_path: ç»“æœæ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºHTMLè·¯å¾„
        """
        if results_path is None:
            results_path = os.path.join(self.save_dir, "reports", "evaluation_results.json")
        
        if not os.path.exists(results_path):
            logger.error("ç»“æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
            return
        
        with open(results_path, "r", encoding='utf-8') as f:
            results = json.load(f)
        
        if output_path is None:
            output_path = os.path.join(self.save_dir, "reports", "evaluation_report.html")
        
        # ç”ŸæˆHTMLå†…å®¹
        html_content = f"""
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>é»‘åå•ç³»ç»Ÿè¯„ä¼°æŠ¥å‘Š</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
                .container {{ max-width: 1200px; margin: 0 auto; background-color: white; padding: 20px; box-shadow: 0 0 10px rgba(0,0,0,0.1); }}
                h1 {{ color: #333; border-bottom: 3px solid #007bff; padding-bottom: 10px; }}
                h2 {{ color: #555; margin-top: 30px; }}
                .metrics {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 15px; margin: 20px 0; }}
                .metric-card {{ background-color: #f8f9fa; padding: 15px; border-radius: 5px; border-left: 4px solid #007bff; }}
                .metric-value {{ font-size: 24px; font-weight: bold; color: #007bff; }}
                .metric-label {{ font-size: 14px; color: #666; margin-top: 5px; }}
                .plot-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(400px, 1fr)); gap: 20px; margin: 20px 0; }}
                .plot-item {{ text-align: center; }}
                .plot-item img {{ max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 5px; }}
                .summary {{ background-color: #e9ecef; padding: 15px; border-radius: 5px; margin: 20px 0; }}
                .error-list {{ background-color: #fff3cd; padding: 15px; border-radius: 5px; margin: 10px 0; }}
            </style>
        </head>
        <body>
            <div class="container">
                <h1>ğŸ¯ é»‘åå•ç³»ç»Ÿæ€§èƒ½è¯„ä¼°æŠ¥å‘Š</h1>
                <p><strong>è¯„ä¼°æ—¶é—´:</strong> {results['timestamp']}</p>
                <p><strong>å†³ç­–é˜ˆå€¼:</strong> {results['threshold']}</p>
                
                <h2>ğŸ“Š æ ¸å¿ƒæŒ‡æ ‡</h2>
                <div class="metrics">
                    <div class="metric-card">
                        <div class="metric-value">{results['metrics']['accuracy']:.2f}%</div>
                        <div class="metric-label">å‡†ç¡®ç‡ (Accuracy)</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">{results['metrics']['precision']:.2f}%</div>
                        <div class="metric-label">ç²¾ç¡®ç‡ (Precision)</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">{results['metrics']['recall']:.2f}%</div>
                        <div class="metric-label">å¬å›ç‡ (Recall)</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">{results['metrics']['f1_score']:.2f}%</div>
                        <div class="metric-label">F1åˆ†æ•°</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">{results['metrics']['false_positive_rate']:.2f}%</div>
                        <div class="metric-label">è¯¯æŠ¥ç‡ (FPR)</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">{results['metrics']['false_negative_rate']:.2f}%</div>
                        <div class="metric-label">æ¼æŠ¥ç‡ (FNR)</div>
                    </div>
                </div>
                
                <div class="summary">
                    <h3>ğŸ“ˆ ç»Ÿè®¡æ‘˜è¦</h3>
                    <p><strong>æ€»æ ·æœ¬æ•°:</strong> {results['metrics']['total_samples']:,}</p>
                    <p><strong>æ­£æ ·æœ¬æ•° (åº”åœ¨é»‘åå•):</strong> {results['metrics']['positive_samples']:,}</p>
                    <p><strong>è´Ÿæ ·æœ¬æ•° (ä¸åº”åœ¨é»‘åå•):</strong> {results['metrics']['negative_samples']:,}</p>
                    <p><strong>å¹³å‡è¯†åˆ«æ—¶é—´:</strong> {results['metrics']['avg_time_per_image_ms']:.2f}ms/å¼ </p>
                    {'<p><strong>ROC-AUC:</strong> ' + str(round(results['metrics'].get('roc_auc', 0), 3)) + '</p>' if 'roc_auc' in results['metrics'] else ''}
                    {'<p><strong>PR-AUC:</strong> ' + str(round(results['metrics'].get('pr_auc', 0), 3)) + '</p>' if 'pr_auc' in results['metrics'] else ''}
                </div>
                
                <h2>ğŸ“‰ å¯è§†åŒ–å›¾è¡¨</h2>
                <div class="plot-grid">
                    <div class="plot-item">
                        <h3>æ··æ·†çŸ©é˜µ</h3>
                        <img src="../plots/confusion_matrix.png" alt="æ··æ·†çŸ©é˜µ">
                    </div>
                    <div class="plot-item">
                        <h3>ROCæ›²çº¿</h3>
                        <img src="../plots/roc_curve.png" alt="ROCæ›²çº¿">
                    </div>
                    <div class="plot-item">
                        <h3>PRæ›²çº¿</h3>
                        <img src="../plots/pr_curve.png" alt="PRæ›²çº¿">
                    </div>
                    <div class="plot-item">
                        <h3>ç›¸ä¼¼åº¦åˆ†å¸ƒ</h3>
                        <img src="../plots/similarity_distribution.png" alt="ç›¸ä¼¼åº¦åˆ†å¸ƒ">
                    </div>
                </div>
                
                <h2>âš ï¸ é”™è¯¯åˆ†æ</h2>
                <div class="error-list">
                    <h3>å‡é˜³æ€§ (è¯¯æŠ¥)</h3>
                    <p>æ•°é‡: {sum(1 for s in results['negative_samples'] if not s['correct'])}/{len(results['negative_samples'])}</p>
                </div>
                <div class="error-list">
                    <h3>å‡é˜´æ€§ (æ¼æŠ¥)</h3>
                    <p>æ•°é‡: {sum(1 for s in results['positive_samples'] if not s['correct'])}/{len(results['positive_samples'])}</p>
                </div>
                
                <hr>
                <p style="text-align: center; color: #666; font-size: 12px;">
                    æŠ¥å‘Šç”±é»‘åå•è¯„ä¼°ç³»ç»Ÿç”Ÿæˆ
                </p>
            </div>
        </body>
        </html>
        """
        
        with open(output_path, "w", encoding='utf-8') as f:
            f.write(html_content)
        
        logger.info(f"HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")


if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹
    evaluator = BlacklistEvaluator(save_dir="evaluation_results")
    
    # 1. åŸºæœ¬è¯„ä¼°
    results = evaluator.evaluate_test_set("test_set_info.json", generate_plots=True)
    
    # 2. é”™è¯¯åˆ†æ
    evaluator.analyze_errors()
    
    # 3. æœç´¢æœ€ä¼˜é˜ˆå€¼
    print("\n" + "="*70)
    print("å¼€å§‹æœç´¢æœ€ä¼˜é˜ˆå€¼...")
    threshold_results, best_threshold, best_metrics = evaluator.search_optimal_threshold(
        threshold_range=(0.3, 0.7), step=0.01
    )
    
    # 4. ç”ŸæˆHTMLæŠ¥å‘Š
    evaluator.generate_html_report()
    
    print("\n" + "="*70)
    print("âœ… è¯„ä¼°å®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³ evaluation_results ç›®å½•")
    print("ğŸ“‚ ç›®å½•ç»“æ„:")
    print("   â”œâ”€â”€ plots/          # å¯è§†åŒ–å›¾è¡¨")
    print("   â”œâ”€â”€ reports/        # è¯„ä¼°æŠ¥å‘Š")
    print("   â””â”€â”€ evaluation_report.html  # äº¤äº’å¼HTMLæŠ¥å‘Š")
    print("="*70)
